Azure ETL Pipeline: Data Extraction, Transformation & Load

ğŸš€ Project Overview
This project implements an end-to-end ETL pipeline using Azure Data Factory (ADF), Azure Databricks, and PySpark. The pipeline extracts data from SQL Server and GitHub, processes it using Medallion Architecture (Bronze, Silver, Gold layers), and supports incremental data loading using SCD Type 1 (Upsert). The solution is designed to ensure data reliability, scalability, and optimized performance for analytics workloads.

 Architecture and Workflow

ğŸ“‚ Data Sources
â€¢ SQL Server is used as the structured relational database source.
â€¢ GitHub is used as a source for extracting raw JSON data.

âš™ï¸ Pipeline Workflow
â€¢ Data ingestion is performed using Azure Data Factory to extract data from source systems.
â€¢ Raw data is copied into staging tables for further processing.
â€¢ Incremental load management is handled using SCD Type 1 Upsert logic.
â€¢ Data transformation is performed in Azure Databricks using PySpark and Delta Lake.
â€¢ Final processed data is stored in fact and dimension tables optimized for reporting and analytics.

ğŸª™ Medallion Architecture Implementation
â€¢ Bronze layer stores raw ingested data without transformation.
â€¢ Silver layer contains cleaned and structured data after transformation.
â€¢ Gold layer contains aggregated and analytics-ready data used for reporting and business insights.

ğŸ› ï¸ Technologies Used
â€¢ Azure Data Factory is used for data orchestration and pipeline execution.
â€¢ Azure Databricks is used for data transformation using PySpark.
â€¢ Delta Lake and Delta Tables are used for optimized storage, ACID transactions, and schema evolution.
â€¢ SQL Server is used for structured data storage.

â­ Key Features
â€¢ Incremental data loading is implemented using a watermark column.
â€¢ SCD Type 1 Upsert logic is used for efficient record updates.
â€¢ Pipeline execution is optimized using parallel processing.
â€¢ Error handling and schema evolution are implemented using Delta Tables.

ğŸ§© Challenges and Solutions

ğŸ“Œ Handling Incremental Loads
â€¢ A watermark table is implemented to track the last load timestamp.
â€¢ SCD Type 1 Upsert ensures efficient updates and avoids duplicate records.

ğŸ“Œ Parallel Execution Issues
â€¢ Data conflicts were resolved by implementing proper merge logic and partition strategies.
â€¢ Schema evolution was managed effectively using Delta Lake features.

